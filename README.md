# ðŸŒ Daily website scraper with â€œdiffâ€ alerts 

## ðŸš€ Project Overview
The **Daily Website Diff Scraper** is an automated website monitoring tool designed
to crawl a target domain daily, track changes in content, and notify the user via
**email alerts** and a **real-time dashboard**.

The scraper stores snapshots of web page content hashes and uses a differential
algorithm to categorize changes as:

- **NEW** â€“ Newly discovered pages
- **CHANGED** â€“ Existing pages with updated content
- **REMOVED** â€“ Pages that no longer exist

The system consists of a polite web crawler, a SQLite database for persistence,
a diff engine, an email alert module, and a Streamlit-based dashboard.

---

## âœ¨ Features
- **Full-Domain Crawl**  
  Recursively crawls all pages within the specified domain.

- **Change Detection**  
  Uses **MD5 hashing** to detect content changes between crawls.

- **Email Alerts**  
  Sends a daily summary email showing counts of new, changed, and removed pages.

- **Interactive Dashboard**  
  A **Streamlit dashboard** to visualize crawl results, change history, and alerts.

- **Polite Crawling**  
  Includes a **1-second delay** between requests to respect server load.

---

## ðŸ› ï¸ Libraries Used
The project uses the following Python libraries:

- **requests** â€“ Fetch web pages  
- **beautifulsoup4** â€“ Parse HTML and extract text  
- **sqlite3** â€“ Persistent storage  
- **schedule** â€“ Daily task scheduling  
- **smtplib**, **email.mime** â€“ Email notifications  
- **pandas** â€“ Data handling for dashboard  
- **streamlit** â€“ Interactive web dashboard  

---

## ðŸ“‚ Project Structure
```text
daily-website-diff-scraper
â”‚
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ differ.py
â”‚   â”œâ”€â”€ email_alert.py
â”‚   â””â”€â”€ scheduler.py
â”‚
â”œâ”€â”€ data
â”‚   â””â”€â”€ scraper.db
â”‚
â”œâ”€â”€ tests
â”‚   â””â”€â”€ test_diff.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/daily-website-diff-scraper.git
cd daily-website-diff-scraper
```

### 2ï¸âƒ£ Create a Virtual Environment (Recommended)
Run the following command to create the environment:

```Bash
python -m venv .venv
```

### Activate the virtual environment:
#### Linux / macOS:

``` Bash
source .venv/bin/activate
```
#### Windows:
``` Bash
.venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies
Install the required packages using pip:
``` Bash
pip install -r requirements.txt
```

### âœ‰ï¸ Email Configuration
Open src/email_alert.py and update the following variables:
```Python
EMAIL = "your_email@gmail.com"
APP_PASSWORD = "your_16_char_app_password"
```

### â–¶ï¸ How to Run the Project
- Aï¸. Run Once (Testing)
To run the scraper a single time for testing purposes:
```Bash
python main.py
```
- Bï¸. Run as a Daily Scheduler
To keep the script running on a schedule:
```Bash
python src/scheduler.py
```
Note: If main.py already calls run_once(), use main.py as the primary entry point.

### ðŸ“Š Run the Dashboard
To launch the visual dashboard:
``` Bash
streamlit run dashboard.py
```
The dashboard shows:
- Change Summary
- Email alerts
- Crawled Pages
- Change history

## ðŸ—„ï¸ Database Schema (data/scraper.db)

### ðŸ“„ pages
Stores the latest snapshot of crawled URLs.

| Column | Description |
| :--- | :--- |
| **url** | Page URL (Primary Key) |
| **content_hash** | MD5 content hash |

### ðŸ”„ changes
Temporary table holding detected changes.

| Column | Description |
| :--- | :--- |
| **url** | Page URL |
| **change_type** | NEW / CHANGED / REMOVED |

### ðŸ“§ alerts
Stores crawl and email history.

| Column | Description |
| :--- | :--- |
| **message** | Summary message |
| **timestamp** | Event time |

**Example message:**
> NEW: 0 | CHANGED: 8 | REMOVED: 0 | Email sent successfully

---

## ðŸ§ª Unit Testing

- **Unit tests implemented using pytest**
- **Core diff logic verified using automated test cases**
- Tests ensure correct detection of **NEW**, **CHANGED**, and **REMOVED** pages
- Test files are located in the `tests/` directory

Run tests using:
```bash
pytest
```
